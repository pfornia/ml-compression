{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Transfers\n",
    "\n",
    "1) Set up a \"branched\" network, where ae and classifier share bottom layers. \n",
    "\n",
    "Some useful code: https://github.com/deep-diver/CIFAR10-VGG19-Tensorflow/blob/master/CIFAR10-transfer-learning-tensornets.ipynb\n",
    "\n",
    "tensorboard tutorial: https://github.com/martinwicke/tf-dev-summit-tensorboard-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Architectures\n",
    "\n",
    "### NN 1 (MNIST AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# Now 14x14x16\n",
    "conv2 = tf.layers.conv2d(maxpool1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "#maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "encoded = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same', name='encoded')\n",
    "# Now 7x7x8\n",
    "#conv3 = tf.layers.conv2d(maxpool2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "#encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# Now 4x4x8\n",
    "\n",
    "\n",
    "#Need some kinda of compression step here...\n",
    "\n",
    "\n",
    "\n",
    "### Decoder\n",
    "#upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
    "# Now 7x7x8\n",
    "#conv4 = tf.layers.conv2d(upsample1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "#upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
    "upsample2 = tf.image.resize_nearest_neighbor(encoded, (14,14))\n",
    "# Now 14x14x8\n",
    "conv5 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
    "# Now 28x28x8\n",
    "conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits, name='loss')\n",
    "cost = tf.reduce_mean(loss, name='cost')\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost, name='opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 200\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(mnist.train.num_examples//batch_size, \"batches\")\n",
    "    for ii in range(mnist.train.num_examples//batch_size):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: imgs,\n",
    "                                                         targets_: imgs})\n",
    "\n",
    "        if ii%50==0: print(\"Epoch: {}/{}, batch {}...\".format(e+1, epochs, ii), \n",
    "                           \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1?\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.6829\n",
    "Epoch: 1/1, batch 50... Training loss: 0.1782\n",
    "Epoch: 1/1, batch 100... Training loss: 0.1204\n",
    "Epoch: 1/1, batch 150... Training loss: 0.1025\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0936\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0897\n",
    "\n",
    "Round 2:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.7201\n",
    "Epoch: 1/1, batch 50... Training loss: 0.1020\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0953\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0927\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0912\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0942\n",
    "\n",
    "Round 3:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.1719\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0925\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0946\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0922\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0907\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0880\n",
    "\n",
    "Round 4:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.1090\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0894\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0893\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0869\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0858\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0840\n",
    "\n",
    "Round 5:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.0990\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0851\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0867\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0824\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0832\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = mnist.test.images[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, try to fit a classifier on top of encoded layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_targets_lab_ = tf.placeholder(tf.int32, (None), name='class_targets_lab_')\n",
    "\n",
    "class_flat1 = tf.layers.flatten(encoded)\n",
    "#dense1 = tf.layers.dense(flat1, 128, activation=tf.nn.relu)\n",
    "# Now 1*128\n",
    "class_drop1 = tf.layers.dropout(class_flat1, rate = 0.5)\n",
    "class_dense2 = tf.layers.dense(class_drop1, 64, activation=tf.nn.relu)\n",
    "# Now 1*64\n",
    "class_drop2 = tf.layers.dropout(class_dense2, rate = 0.5)\n",
    "class_final = tf.layers.dense(class_drop2, 10, activation=tf.nn.softmax)\n",
    "# 1*10\n",
    "\n",
    "class_one_hot_labs = tf.one_hot(class_targets_lab_,10)\n",
    "\n",
    "#loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=one_hot_labs, logits=final)\n",
    "class_loss1 = tf.nn.softmax_cross_entropy_with_logits(labels=class_one_hot_labs, logits=class_final)\n",
    "class_cost1 = tf.reduce_mean(class_loss1)\n",
    "class_opt1 = tf.train.AdamOptimizer(0.001).minimize(class_cost1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize only the uninitialized variables (don't overwrite trained variables)\n",
    "\n",
    "https://stackoverflow.com/questions/35164529/in-tensorflow-is-there-any-way-to-just-initialize-uninitialised-variables/35618160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.get_variable([sess.run(tf.report_uninitialized_variables(tf.all_variables()))[0]])\n",
    "\n",
    "#uninitialized_variables = list(tf.get_variable(name) for name in\n",
    "tf.initialize_variables([sess.run(tf.report_uninitialized_variables(tf.all_variables()))[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #2\n",
    "\n",
    "### Also failed... BUT, I can just initialize everything up front! Genius!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninitialized_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uninitialized_vars = []\n",
    "#for var in tf.all_variables():\n",
    "#    try:\n",
    "#        sess.run(var)\n",
    "#    except tf.errors.FailedPreconditionError:\n",
    "#        uninitialized_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.initialize_variables(uninitialized_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the thing\n",
    "epochs = 1\n",
    "batch_size = 200\n",
    "#Maybe I don't initialize since the params are already pre-trained??\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#sess.run(class_targets_lab_.initializer)\n",
    "#sess.run(tf.variables_initializer([class_flat1]))\n",
    "#sess.run(class_flat1.initializer)\n",
    "#sess.run(class_drop1.initializer)\n",
    "#sess.run(class_dense2.initializer)\n",
    "#sess.run(class_drop2.initializer)\n",
    "#sess.run(class_final.initializer)\n",
    "#sess.run(class_one_hot_labs.initializer)\n",
    "#sess.run(class_loss1.initializer)\n",
    "#sess.run(class_cost1.initializer)\n",
    "#sess.run(class_opt1.initializer)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(mnist.train.num_examples//batch_size, \"batches\")\n",
    "    for ii in range(mnist.train.num_examples//batch_size):\n",
    "        imgs = mnist.train.images[ii*batch_size : (ii+1)*batch_size].reshape((-1, 28, 28, 1))\n",
    "        labs = mnist.train.labels[ii*batch_size : (ii+1)*batch_size]\n",
    "        #print(labs.shape)\n",
    "        #imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        batch_cost, _ = sess.run([class_cost1, class_opt1], feed_dict={inputs_: imgs,\n",
    "                                                           class_targets_lab_: labs})\n",
    "\n",
    "        if ii%50==0: print(\"Epoch: {}/{}, batch {}...\".format(e+1, epochs, ii), \n",
    "                           \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 2.3055\n",
    "Epoch: 1/1, batch 50... Training loss: 1.8235\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6725\n",
    "Epoch: 1/1, batch 150... Training loss: 1.6407\n",
    "Epoch: 1/1, batch 200... Training loss: 1.6555\n",
    "Epoch: 1/1, batch 250... Training loss: 1.6257\n",
    "\n",
    "Round 2\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6475\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6305\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6143\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5814\n",
    "Epoch: 1/1, batch 200... Training loss: 1.6306\n",
    "Epoch: 1/1, batch 250... Training loss: 1.6183\n",
    "\n",
    "Round 3:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6529\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6107\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6054\n",
    "Epoch: 1/1, batch 150... Training loss: 1.6203\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5574\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5753\n",
    "\n",
    "Round 4:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6008\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6237\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6226\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5701\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5743\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5555\n",
    "\n",
    "Round 5:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6018\n",
    "Epoch: 1/1, batch 50... Training loss: 1.5357\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6070\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5750\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5945\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_preds = sess.run(class_final, feed_dict = {inputs_: mnist.test.images.reshape(-1,28,28,1)})\n",
    "test_preds_cat = [x.argmax() for x in test_preds]\n",
    "sum(test_preds_cat == mnist.test.labels)/len(test_preds_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking promising! After alternating, I should still \"freeze\" the encoder and fine tune decoder and classifier portions for accuracy boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models for later use\n",
    "\n",
    "https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"./ml-compression/models/model1\")\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, tf.train.latest_checkpoint('./ml-compression/models/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
