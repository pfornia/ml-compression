{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Transfers\n",
    "\n",
    "1) Set up a \"branched\" network, where ae and classifier share bottom layers. \n",
    "\n",
    "Some useful code: https://github.com/deep-diver/CIFAR10-VGG19-Tensorflow/blob/master/CIFAR10-transfer-learning-tensornets.ipynb\n",
    "\n",
    "tensorboard tutorial: https://github.com/martinwicke/tf-dev-summit-tensorboard-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Architectures\n",
    "\n",
    "### NN 1 (MNIST AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# Now 14x14x16\n",
    "conv2 = tf.layers.conv2d(maxpool1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "#maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "encoded = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same', name='encoded')\n",
    "# Now 7x7x8\n",
    "#conv3 = tf.layers.conv2d(maxpool2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "#encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# Now 4x4x8\n",
    "\n",
    "\n",
    "#Need some kinda of compression step here...\n",
    "\n",
    "\n",
    "\n",
    "### Decoder\n",
    "#upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
    "# Now 7x7x8\n",
    "#conv4 = tf.layers.conv2d(upsample1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "#upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
    "upsample2 = tf.image.resize_nearest_neighbor(encoded, (14,14))\n",
    "# Now 14x14x8\n",
    "conv5 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
    "# Now 28x28x8\n",
    "conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits, name='loss')\n",
    "cost = tf.reduce_mean(loss, name='cost')\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost, name='opt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN2 MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-5e3480082ad5>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_targets_lab_ = tf.placeholder(tf.int32, (None), name='class_targets_lab_')\n",
    "\n",
    "class_flat1 = tf.layers.flatten(encoded)\n",
    "#dense1 = tf.layers.dense(flat1, 128, activation=tf.nn.relu)\n",
    "# Now 1*128\n",
    "class_drop1 = tf.layers.dropout(class_flat1, rate = 0.5)\n",
    "class_dense2 = tf.layers.dense(class_drop1, 64, activation=tf.nn.relu)\n",
    "# Now 1*64\n",
    "class_drop2 = tf.layers.dropout(class_dense2, rate = 0.5)\n",
    "class_final = tf.layers.dense(class_drop2, 10, activation=tf.nn.softmax)\n",
    "# 1*10\n",
    "\n",
    "class_one_hot_labs = tf.one_hot(class_targets_lab_,10)\n",
    "\n",
    "#loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=one_hot_labs, logits=final)\n",
    "class_loss1 = tf.nn.softmax_cross_entropy_with_logits(labels=class_one_hot_labs, logits=class_final)\n",
    "class_cost1 = tf.reduce_mean(class_loss1)\n",
    "class_opt1 = tf.train.AdamOptimizer(0.001).minimize(class_cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.variable_scope('dense', reuse=True):\n",
    "#    print(sess.run(tf.get_variable('kernel')).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing Functions\n",
    "### Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(data, epochs = 1, batch_size = 200):\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(data.train.num_examples//batch_size, \"batches\")\n",
    "        for ii in range(data.train.num_examples//batch_size):\n",
    "            batch = data.train.next_batch(batch_size)\n",
    "            imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "            batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: imgs,\n",
    "                                                             targets_: imgs})\n",
    "\n",
    "            if ii%50==0: print(\"Epoch: {}/{}, batch {}...\".format(e+1, epochs, ii), \n",
    "                               \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference AE (Viz version)\n",
    "To Do: look up better compression metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_ae_viz(data):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "    in_imgs = data.test.images[:10]\n",
    "    reconstructed = sess.run(decoded, feed_dict={inputs_: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "    for images, row in zip([in_imgs, reconstructed], axes):\n",
    "        for img, ax in zip(images, row):\n",
    "            ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_class(data, epochs = 1, batch_size = 200):\n",
    "    for e in range(epochs):\n",
    "        print(data.train.num_examples//batch_size, \"batches\")\n",
    "        for ii in range(data.train.num_examples//batch_size):\n",
    "            imgs = data.train.images[ii*batch_size : (ii+1)*batch_size].reshape((-1, 28, 28, 1))\n",
    "            labs = data.train.labels[ii*batch_size : (ii+1)*batch_size]\n",
    "            #print(labs.shape)\n",
    "            #imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "            batch_cost, _ = sess.run([class_cost1, class_opt1], feed_dict={inputs_: imgs,\n",
    "                                                               class_targets_lab_: labs})\n",
    "\n",
    "            if ii%50==0: print(\"Epoch: {}/{}, batch {}...\".format(e+1, epochs, ii), \n",
    "                               \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_class(data):\n",
    "    # Test accuracy\n",
    "    test_preds = sess.run(class_final, feed_dict = {inputs_: data.test.images.reshape(-1,28,28,1)})\n",
    "    test_preds_cat = [x.argmax() for x in test_preds]\n",
    "    print(sum(test_preds_cat == data.test.labels)/len(test_preds_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(data):\n",
    "    return(sess.run(encoded, feed_dict = {inputs_: data.images.reshape(-1,28,28,1)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP: Train AE Frozen\n",
    "Is this even possible in TF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Compress (i.e., freeze bottleneck output)\n",
    "    print(\"Compressing Images...\")\n",
    "    compressed = compress(mnist.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_ae_frozen(data, epochs = 1, batch_size = 200):\n",
    "#    #Compress (i.e., freeze bottleneck output)\n",
    "#    print(\"Compressing Images...\")\n",
    "#    compressed = compress(data.train)\n",
    "\n",
    "    #Train top-half layers only\n",
    "    print(\"Retraining Upper layers...\")\n",
    "    for e in range(epochs):\n",
    "        print(data.train.num_examples//batch_size, \"batches\")\n",
    "        for ii in range(data.train.num_examples//batch_size):\n",
    "            batch = data.train.next_batch(batch_size)\n",
    "            imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "            imgs_comp = compressed[ii*batch_size : (ii+1)*batch_size]\n",
    "            batch_cost, _ = sess.run([cost, opt], feed_dict={encoded: imgs_comp,\n",
    "                                                             targets_: imgs})\n",
    "\n",
    "            if ii%50==0: print(\"Epoch: {}/{}, batch {}...\".format(e+1, epochs, ii), \n",
    "                               \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ae_frozen(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "### Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)\n",
    "#To Do: see mnist warning the first time this is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize from file or randomly\n",
    "\n",
    "!!! WARNING: restore not working !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_fresh = True\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100000)\n",
    "\n",
    "if start_fresh:\n",
    "    tf.set_random_seed(1)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "else:\n",
    "    saver.restore(sess, '../models/FILENAME')#tf.train.latest_checkpoint('../models/'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_dig(i):\n",
    "    if i<10: return(\"00\" + str(i))\n",
    "    elif i<100: return(\"0\" + str(i))\n",
    "    else: return(str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.6985\n",
      "Epoch: 1/1, batch 50... Training loss: 0.2182\n",
      "Epoch: 1/1, batch 100... Training loss: 0.1464\n",
      "Epoch: 1/1, batch 150... Training loss: 0.1078\n",
      "Epoch: 1/1, batch 200... Training loss: 0.1011\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0968\n",
      "0.0993\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 2.3092\n",
      "Epoch: 1/1, batch 50... Training loss: 1.8038\n",
      "Epoch: 1/1, batch 100... Training loss: 1.6430\n",
      "Epoch: 1/1, batch 150... Training loss: 1.6310\n",
      "Epoch: 1/1, batch 200... Training loss: 1.6940\n",
      "Epoch: 1/1, batch 250... Training loss: 1.6358\n",
      "0.8441\n",
      "     None\n",
      "Round 1\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.2837\n",
      "Epoch: 1/1, batch 50... Training loss: 0.1028\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0947\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0947\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0900\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0914\n",
      "0.8313\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.6254\n",
      "Epoch: 1/1, batch 50... Training loss: 1.5998\n",
      "Epoch: 1/1, batch 100... Training loss: 1.5385\n",
      "Epoch: 1/1, batch 150... Training loss: 1.5266\n",
      "Epoch: 1/1, batch 200... Training loss: 1.5168\n",
      "Epoch: 1/1, batch 250... Training loss: 1.5465\n",
      "0.9471\n",
      "     None\n",
      "Round 2\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.2203\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0930\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0877\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0910\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0870\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0866\n",
      "0.939\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5566\n",
      "Epoch: 1/1, batch 50... Training loss: 1.5212\n",
      "Epoch: 1/1, batch 100... Training loss: 1.5112\n",
      "Epoch: 1/1, batch 150... Training loss: 1.5080\n",
      "Epoch: 1/1, batch 200... Training loss: 1.5139\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4971\n",
      "0.9657\n",
      "     None\n",
      "Round 3\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.1340\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0852\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0854\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0863\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0839\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0865\n",
      "0.9487\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5240\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4767\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4996\n",
      "Epoch: 1/1, batch 150... Training loss: 1.5097\n",
      "Epoch: 1/1, batch 200... Training loss: 1.5049\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4896\n",
      "0.9683\n",
      "     None\n",
      "Round 4\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0952\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0868\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0854\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0824\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0842\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0844\n",
      "0.9534\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4984\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4856\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4973\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4909\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4829\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4860\n",
      "0.9693\n",
      "     None\n",
      "Round 5\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0901\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0809\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0823\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0834\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0814\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0818\n",
      "0.957\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4968\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4990\n",
      "Epoch: 1/1, batch 100... Training loss: 1.5123\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4964\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4886\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4819\n",
      "0.9767\n",
      "     None\n",
      "Round 6\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0835\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0820\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0792\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0772\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0796\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0807\n",
      "0.9601\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5204\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4706\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4918\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4793\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4879\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4851\n",
      "0.9757\n",
      "     None\n",
      "Round 7\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0825\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0797\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0779\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0795\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0787\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0812\n",
      "0.9618\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5226\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4925\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4834\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4907\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4847\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4706\n",
      "0.9746\n",
      "     None\n",
      "Round 8\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0861\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0770\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0783\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0804\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0783\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0779\n",
      "0.9539\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4997\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4944\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4714\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4855\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4794\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4884\n",
      "0.976\n",
      "     None\n",
      "Round 9\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0859\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0776\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0786\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0734\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0740\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0778\n",
      "0.9698\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4997\n",
      "Epoch: 1/1, batch 50... Training loss: 1.5047\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4893\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4889\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4834\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4706\n",
      "0.9792\n",
      "     None\n",
      "Round 10\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0789\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0763\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0735\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0777\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0762\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0765\n",
      "0.9597\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5008\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4749\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4864\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4740\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4817\n",
      "Epoch: 1/1, batch 250... Training loss: 1.5048\n",
      "0.9794\n",
      "     None\n",
      "Round 11\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0814\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0751\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0748\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0759\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0750\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0751\n",
      "0.966\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4916\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4810\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4844\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4836\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4725\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4974\n",
      "0.9786\n",
      "     None\n",
      "Round 12\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0804\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0778\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0756\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0739\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0750\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9644\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4989\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4846\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4731\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4874\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4892\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4829\n",
      "0.9818\n",
      "     None\n",
      "Round 13\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0812\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0741\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0757\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0738\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0751\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0749\n",
      "0.9624\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4896\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4710\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4779\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4953\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4770\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4831\n",
      "0.98\n",
      "     None\n",
      "Round 14\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0791\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0753\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0737\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0744\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0752\n",
      "0.958\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4928\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4832\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4871\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4930\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4729\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4844\n",
      "0.9814\n",
      "     None\n",
      "Round 15\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0762\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0741\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0750\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0754\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0732\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0740\n",
      "0.9649\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5155\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4765\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4677\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4668\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4791\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4800\n",
      "0.9764\n",
      "     None\n",
      "Round 16\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0802\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0731\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0742\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0721\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0733\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0736\n",
      "0.9519\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5152\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4847\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4710\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4779\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4724\n",
      "Epoch: 1/1, batch 250... Training loss: 1.5030\n",
      "0.9826\n",
      "     None\n",
      "Round 17\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0778\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0735\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0734\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0732\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0747\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0724\n",
      "0.9762\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4866\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4810\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4823\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4862\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4753\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4742\n",
      "0.9818\n",
      "     None\n",
      "Round 18\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0789\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0748\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0726\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0730\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0738\n",
      "0.967\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4787\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4685\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4714\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4764\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4835\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4759\n",
      "0.9776\n",
      "     None\n",
      "Round 19\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0809\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0744\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0726\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0758\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0734\n",
      "0.9489\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5009\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4793\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4764\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4771\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4692\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4852\n",
      "0.9829\n",
      "     None\n",
      "Round 20\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0823\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0737\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0745\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0749\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0706\n",
      "0.9704\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4869\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4877\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4660\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4948\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4899\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4805\n",
      "0.9835\n",
      "     None\n",
      "Round 21\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0826\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0734\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0718\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0743\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0713\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0723\n",
      "0.9745\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4864\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4828\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4742\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4815\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4790\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4859\n",
      "0.9806\n",
      "     None\n",
      "Round 22\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0771\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0743\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0724\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0712\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0711\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0717\n",
      "0.9662\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5080\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4672\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4807\n",
      "Epoch: 1/1, batch 150... Training loss: 1.5097\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4679\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4812\n",
      "0.9826\n",
      "     None\n",
      "Round 23\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0805\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0722\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0734\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0745\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0732\n",
      "0.9704\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4791\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4966\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4667\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4817\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4980\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4757\n",
      "0.9822\n",
      "     None\n",
      "Round 24\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0792\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0733\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0739\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0712\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0720\n",
      "0.9676\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5097\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4779\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4876\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4790\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, batch 250... Training loss: 1.4731\n",
      "0.981\n",
      "     None\n",
      "Round 25\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0813\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0720\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0720\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0715\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0726\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0743\n",
      "0.9679\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4857\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4757\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4778\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4833\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4759\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4711\n",
      "0.9838\n",
      "     None\n",
      "Round 26\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0790\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0718\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0741\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0704\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0707\n",
      "0.9725\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4884\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4652\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4637\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4850\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4813\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4715\n",
      "0.9819\n",
      "     None\n",
      "Round 27\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0786\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0719\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0737\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0736\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0702\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0724\n",
      "0.9681\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4959\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4871\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4680\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4749\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4626\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4667\n",
      "0.9833\n",
      "     None\n",
      "Round 28\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0768\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0701\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0730\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0724\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0716\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0705\n",
      "0.9725\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4908\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4762\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4780\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4849\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4775\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4757\n",
      "0.9809\n",
      "     None\n",
      "Round 29\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0784\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0737\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0729\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0690\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0710\n",
      "0.9689\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5005\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4706\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4928\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4663\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4846\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4692\n",
      "0.9841\n",
      "     None\n",
      "Round 30\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0819\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0713\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0715\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0729\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0732\n",
      "0.9756\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4675\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4742\n",
      "Epoch: 1/1, batch 100... Training loss: 1.5066\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4849\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4710\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4727\n",
      "0.9821\n",
      "     None\n",
      "Round 31\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0757\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0736\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0752\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0700\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0700\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0728\n",
      "0.9661\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4951\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4669\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4707\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4703\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4801\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4692\n",
      "0.9814\n",
      "     None\n",
      "Round 32\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0816\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0720\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0725\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0715\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0713\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0715\n",
      "0.9711\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4845\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4663\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4848\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4775\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4760\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4712\n",
      "0.9839\n",
      "     None\n",
      "Round 33\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0742\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0692\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0718\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0691\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0693\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0723\n",
      "0.9745\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4783\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4701\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4621\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4742\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4726\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4767\n",
      "0.9834\n",
      "     None\n",
      "Round 34\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0794\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0712\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0713\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0717\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0728\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0696\n",
      "0.9717\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4971\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4941\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4651\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4808\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4678\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4701\n",
      "0.9829\n",
      "     None\n",
      "Round 35\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0789\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0706\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0709\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0728\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0706\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0720\n",
      "0.9677\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4929\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4688\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4785\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4690\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4680\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4661\n",
      "0.9832\n",
      "     None\n",
      "Round 36\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0792\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0707\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0717\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0697\n",
      "0.9679\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4933\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4807\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4664\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4764\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4703\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4664\n",
      "0.9833\n",
      "     None\n",
      "Round 37\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0780\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0683\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0711\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, batch 200... Training loss: 0.0713\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0724\n",
      "0.9743\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4764\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4718\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4872\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4616\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4734\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4664\n",
      "0.9797\n",
      "     None\n",
      "Round 38\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0872\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0709\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0714\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0712\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0721\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0711\n",
      "0.9642\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4969\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4667\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4697\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4727\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4644\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4712\n",
      "0.9829\n",
      "     None\n",
      "Round 39\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0766\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0714\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0693\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0696\n",
      "0.9699\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4821\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4781\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4865\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4712\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4812\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4617\n",
      "0.9832\n",
      "     None\n",
      "Round 40\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0780\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0724\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0710\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0721\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0711\n",
      "0.9688\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4836\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4795\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4701\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4626\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4746\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4774\n",
      "0.9817\n",
      "     None\n",
      "Round 41\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0783\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0729\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0711\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0719\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0714\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0718\n",
      "0.9682\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5169\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4827\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4685\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4757\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4698\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4716\n",
      "0.9841\n",
      "     None\n",
      "Round 42\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0773\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0727\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0702\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0699\n",
      "0.9724\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4849\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4665\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4636\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4778\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4753\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4622\n",
      "0.9832\n",
      "     None\n",
      "Round 43\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0894\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0694\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0715\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0706\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0699\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0703\n",
      "0.9713\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4874\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4656\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4624\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4727\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4652\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4729\n",
      "0.983\n",
      "     None\n",
      "Round 44\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0762\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0709\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0706\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0715\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0694\n",
      "0.9675\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4772\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4735\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4679\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4781\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4624\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4719\n",
      "0.9835\n",
      "     None\n",
      "Round 45\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0787\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0720\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0705\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0694\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0706\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0713\n",
      "0.9705\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4927\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4791\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4681\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4664\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4723\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4873\n",
      "0.9827\n",
      "     None\n",
      "Round 46\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0840\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0711\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0700\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0704\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0724\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0707\n",
      "0.9601\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5230\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4709\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4669\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4713\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4788\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4653\n",
      "0.9836\n",
      "     None\n",
      "Round 47\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0756\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0732\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0725\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0694\n",
      "0.9685\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4761\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4761\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4685\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4744\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4699\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4738\n",
      "0.9827\n",
      "     None\n",
      "Round 48\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0790\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0720\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0685\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0722\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0702\n",
      "0.9525\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.4805\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4712\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4724\n",
      "Epoch: 1/1, batch 150... Training loss: 1.4630\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4632\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4674\n",
      "0.9818\n",
      "     None\n",
      "Round 49\n",
      "    training AE\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0783\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0703\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0723\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0708\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0689\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0702\n",
      "0.9643\n",
      "     None\n",
      "    training Class\n",
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 1.5155\n",
      "Epoch: 1/1, batch 50... Training loss: 1.4681\n",
      "Epoch: 1/1, batch 100... Training loss: 1.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, batch 150... Training loss: 1.4692\n",
      "Epoch: 1/1, batch 200... Training loss: 1.4672\n",
      "Epoch: 1/1, batch 250... Training loss: 1.4663\n",
      "0.9845\n",
      "     None\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): \n",
    "    \n",
    "    tf.set_random_seed(i*42)\n",
    "    \n",
    "    print(\"Round\", i)\n",
    "    print(\"    training AE\")\n",
    "    train_ae(mnist)\n",
    "    saver.save(sess, \"../models/temp/round%s_ae\"%three_dig(i))\n",
    "    \n",
    "    #Print class Accuracy (should be low)\n",
    "    print(\"    \", infer_class(mnist))\n",
    "    #To do: print AE accuracy\n",
    "    \n",
    "    print(\"    training Class\")\n",
    "    train_class(mnist)\n",
    "    saver.save(sess, \"../models/temp/round%s_class\"%three_dig(i))\n",
    "\n",
    "    #Print class Accuracy (should be high)\n",
    "    print(\"    \", infer_class(mnist))\n",
    "    #To do: print AE accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 batches\n",
      "Epoch: 1/1, batch 0... Training loss: 0.0745\n",
      "Epoch: 1/1, batch 50... Training loss: 0.0717\n",
      "Epoch: 1/1, batch 100... Training loss: 0.0700\n",
      "Epoch: 1/1, batch 150... Training loss: 0.0710\n",
      "Epoch: 1/1, batch 200... Training loss: 0.0701\n",
      "Epoch: 1/1, batch 250... Training loss: 0.0701\n"
     ]
    }
   ],
   "source": [
    "train_ae(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_ae_viz(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class(mnist)\n",
    "saver.save(sess, \"../models/class_v001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0752\n"
     ]
    }
   ],
   "source": [
    "infer_class(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1?\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.6829\n",
    "Epoch: 1/1, batch 50... Training loss: 0.1782\n",
    "Epoch: 1/1, batch 100... Training loss: 0.1204\n",
    "Epoch: 1/1, batch 150... Training loss: 0.1025\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0936\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0897\n",
    "\n",
    "Round 2:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.7201\n",
    "Epoch: 1/1, batch 50... Training loss: 0.1020\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0953\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0927\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0912\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0942\n",
    "\n",
    "Round 3:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.1719\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0925\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0946\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0922\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0907\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0880\n",
    "\n",
    "Round 4:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.1090\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0894\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0893\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0869\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0858\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0840\n",
    "\n",
    "Round 5:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 0.0990\n",
    "Epoch: 1/1, batch 50... Training loss: 0.0851\n",
    "Epoch: 1/1, batch 100... Training loss: 0.0867\n",
    "Epoch: 1/1, batch 150... Training loss: 0.0824\n",
    "Epoch: 1/1, batch 200... Training loss: 0.0832\n",
    "Epoch: 1/1, batch 250... Training loss: 0.0847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 2.3055\n",
    "Epoch: 1/1, batch 50... Training loss: 1.8235\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6725\n",
    "Epoch: 1/1, batch 150... Training loss: 1.6407\n",
    "Epoch: 1/1, batch 200... Training loss: 1.6555\n",
    "Epoch: 1/1, batch 250... Training loss: 1.6257\n",
    "\n",
    "Round 2\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6475\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6305\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6143\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5814\n",
    "Epoch: 1/1, batch 200... Training loss: 1.6306\n",
    "Epoch: 1/1, batch 250... Training loss: 1.6183\n",
    "\n",
    "Round 3:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6529\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6107\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6054\n",
    "Epoch: 1/1, batch 150... Training loss: 1.6203\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5574\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5753\n",
    "\n",
    "Round 4:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6008\n",
    "Epoch: 1/1, batch 50... Training loss: 1.6237\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6226\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5701\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5743\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5555\n",
    "\n",
    "Round 5:\n",
    "\n",
    "300 batches\n",
    "Epoch: 1/1, batch 0... Training loss: 1.6018\n",
    "Epoch: 1/1, batch 50... Training loss: 1.5357\n",
    "Epoch: 1/1, batch 100... Training loss: 1.6070\n",
    "Epoch: 1/1, batch 150... Training loss: 1.5750\n",
    "Epoch: 1/1, batch 200... Training loss: 1.5945\n",
    "Epoch: 1/1, batch 250... Training loss: 1.5624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking promising! After alternating, I should still \"freeze\" the encoder and fine tune decoder and classifier portions for accuracy boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models for later use\n",
    "\n",
    "https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"../models/cheese\")\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, tf.train.latest_checkpoint('../models/cheese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
